{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.218268Z",
     "iopub.status.busy": "2024-01-05T12:26:45.217583Z",
     "iopub.status.idle": "2024-01-05T12:26:45.223527Z",
     "shell.execute_reply": "2024-01-05T12:26:45.222666Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.218223Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import Conv2D, Linear, Embedding\n",
    "from paddle import to_tensor\n",
    "import paddle.nn.functional as F\n",
    "# print(paddle.__version__)\n",
    "\n",
    "import os, zipfile\n",
    "import io, random, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#解压并读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.225803Z",
     "iopub.status.busy": "2024-01-05T12:26:45.224965Z",
     "iopub.status.idle": "2024-01-05T12:26:45.230880Z",
     "shell.execute_reply": "2024-01-05T12:26:45.229828Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.225770Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "src_path=\"data/Rumor_Dataset.zip\"\n",
    "target_path=\"/home/aistudio/data/Chinese_Rumor_Dataset-master\"\n",
    "if(not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.232630Z",
     "iopub.status.busy": "2024-01-05T12:26:45.232087Z",
     "iopub.status.idle": "2024-01-05T12:26:45.424058Z",
     "shell.execute_reply": "2024-01-05T12:26:45.423016Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.232603Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rumor_num=1538\r\n",
      "non_rumor_num=1849\r\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import random\n",
    "import json\n",
    "\n",
    "rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\n",
    "\n",
    "non_rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\n",
    "\n",
    "original_microblog = target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\n",
    "\n",
    "#rumor标签='0',non_rumor标签='1'\n",
    "rumor_label=\"0\"\n",
    "non_rumor_label=\"1\"\n",
    "\n",
    "#统计谣言数据与非谣言数据的总数\n",
    "rumor_num = 0\n",
    "non_rumor_num = 0\n",
    "\n",
    "all_rumor_list = []\n",
    "all_non_rumor_list = []\n",
    "\n",
    "#样本信息统计\n",
    "for rumor_class_dir in rumor_class_dirs: \n",
    "    if(rumor_class_dir != '.DS_Store'):\n",
    "        #遍历谣言数据，并解析\n",
    "        with open(original_microblog + rumor_class_dir, 'r') as f:\n",
    "\t        rumor_content = f.read()\n",
    "        rumor_dict = json.loads(rumor_content)\n",
    "        all_rumor_list.append(rumor_label+\"\\t\"+rumor_dict[\"text\"]+\"\\n\")\n",
    "        rumor_num +=1\n",
    "\n",
    "for non_rumor_class_dir in non_rumor_class_dirs: \n",
    "    if(non_rumor_class_dir != '.DS_Store'):\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r') as f2:\n",
    "\t        non_rumor_content = f2.read()\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\n",
    "        all_non_rumor_list.append(non_rumor_label+\"\\t\"+non_rumor_dict[\"text\"]+\"\\n\")\n",
    "        non_rumor_num +=1\n",
    "        \n",
    "print(\"rumor_num=\"+str(rumor_num))\n",
    "print(\"non_rumor_num=\"+str(non_rumor_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.425675Z",
     "iopub.status.busy": "2024-01-05T12:26:45.425291Z",
     "iopub.status.idle": "2024-01-05T12:26:45.442087Z",
     "shell.execute_reply": "2024-01-05T12:26:45.440893Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.425648Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#全部数据shuffle后写入all_data.txt\n",
    "\n",
    "data_list_path=\"/home/aistudio/data/\"\n",
    "all_data_path=data_list_path + \"data.txt\"\n",
    "\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\n",
    "\n",
    "random.shuffle(all_data_list)\n",
    "\n",
    "#在生成all_data.txt之前，首先将其清空\n",
    "with open(all_data_path, 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate() \n",
    "    \n",
    "with open(all_data_path, 'a') as f:\n",
    "    for data in all_data_list:\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.446002Z",
     "iopub.status.busy": "2024-01-05T12:26:45.445204Z",
     "iopub.status.idle": "2024-01-05T12:26:45.456531Z",
     "shell.execute_reply": "2024-01-05T12:26:45.455666Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.445948Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 生成数据字典\n",
    "def create_dict(data_path, dict_path):\n",
    "    with open(dict_path, 'w') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate() \n",
    "\n",
    "    dict_set = set()\n",
    "    # 读取全部数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 把数据生成一个元组\n",
    "    for line in lines:\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\n",
    "        for s in content:\n",
    "            dict_set.add(s)\n",
    "    # 把元组转换成字典，一个字对应一个数字\n",
    "    dict_list = []\n",
    "    i = 0\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "    # 添加未知字符\n",
    "    dict_txt = dict(dict_list)\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    end_dict = {\"<pad>\": i+1}\n",
    "    dict_txt.update(end_dict)\n",
    "    # 把这些字典保存到本地中\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "\n",
    "        \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.458008Z",
     "iopub.status.busy": "2024-01-05T12:26:45.457690Z",
     "iopub.status.idle": "2024-01-05T12:26:45.467311Z",
     "shell.execute_reply": "2024-01-05T12:26:45.466509Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.457980Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 创建序列化表示的数据,并按照一定比例划分数据集为训练数据train.txt与验证数据val.txt\n",
    "def create_data_list(data_list_path):\n",
    "    #在生成数据之前，首先将val.txt和train.txt清空\n",
    "    with open(os.path.join(data_list_path, 'val.txt'), 'w', encoding='utf-8') as f_eval:\n",
    "        f_eval.seek(0)\n",
    "        f_eval.truncate()\n",
    "        \n",
    "    with open(os.path.join(data_list_path, 'train.txt'), 'w', encoding='utf-8') as f_train:\n",
    "        f_train.seek(0)\n",
    "        f_train.truncate() \n",
    "    \n",
    "    with open(os.path.join(data_list_path, 'dict.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "\n",
    "    with open(os.path.join(data_list_path, 'data.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    maxlen = 0\n",
    "    with open(os.path.join(data_list_path, 'val.txt'), 'a', encoding='utf-8') as f_eval,open(os.path.join(data_list_path, 'train.txt'), 'a', encoding='utf-8') as f_train:\n",
    "        for line in lines:\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\n",
    "            maxlen = max(maxlen, len(words))\n",
    "            label = line.split('\\t')[0]\n",
    "            labs = \"\"\n",
    "            # 每8个抽取一个数据用于验证\n",
    "            if i % 5 == 0:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_eval.write(labs)\n",
    "            else:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_train.write(labs)\n",
    "            i += 1\n",
    "        \n",
    "    print(\"数据列表生成完成！\")\n",
    "    print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.468906Z",
     "iopub.status.busy": "2024-01-05T12:26:45.468339Z",
     "iopub.status.idle": "2024-01-05T12:26:45.705082Z",
     "shell.execute_reply": "2024-01-05T12:26:45.704201Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.468877Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\r\n",
      "数据列表生成完成！\r\n",
      "226\r\n"
     ]
    }
   ],
   "source": [
    "# 把生成的数据列表都放在自己的总类别文件夹中\n",
    "data_root_path = \"/home/aistudio/data/\" \n",
    "data_path = os.path.join(data_root_path, 'data.txt')\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\n",
    "\n",
    "# 创建数据字典\n",
    "create_dict(data_path, dict_path)\n",
    "\n",
    "# 创建数据列表\n",
    "create_data_list(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.707004Z",
     "iopub.status.busy": "2024-01-05T12:26:45.706216Z",
     "iopub.status.idle": "2024-01-05T12:26:45.783872Z",
     "shell.execute_reply": "2024-01-05T12:26:45.782473Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.706973Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\r\n",
      "sentence list id is: ['2608', '1939', '4288', '707', '4296', '3433', '2608', '1192', '1186', '959', '2156', '4388', '2773', '2490', '1152', '3152', '731', '1009', '2816', '1036', '1476', '2836', '4262', '640', '3221', '1085', '3378', '56', '298', '3945', '4031', '1733', '2309', '4188', '488', '1852', '4313', '1351', '577', '4262', '675', '210', '2773', '1152', '24', '2156', '1553', '1361', '1042', '396', '1733', '2638', '1941', '24', '1351', '2156', '406', '1602', '1234', '121', '769', '2656', '3380', '1085', '3378', '56', '3825', '2838', '3880', '265', '2156', '788', '2755', '676', '3583', '3982', '1245', '915', '1085', '436', '406', '488', '1676', '298', '675', '210', '3825', '2341', '4198', '2448', '1671', '2374', '2448', '3381', '2633', '2448', '3346', '1226', '2448', '1338', '254', '2037', '2448', '1981', '2577', '2843', '2448', '1952', '2770', '2194', '2448', '3191', '863', '2448', '361', '2712', '2448', '1906', '2374', '2448', '3381', '1069', '1085', '1852', '1152', '3581', '155\r\n",
      "sentence list is:  # 肯 定 没 人 理 #   小 透 明 啥 都 不 会 就 最 后 几 分 钟 凑 个 热 闹 。 转 发 的 姑 娘 ， 接 下 去 我 每 到 一 个 地 方 都 会 寄 明 信 片 给 你 ， 大 概 寄 到 明 年 光 棍 节 为 止 吧 。 转 发 有 效 期 是 明 天 上 午 九 点 之 前 。 今 年 去 过 的 地 方 有 乌 镇 、 杭 州 、 武 汉 、 沈 阳 、 长 白 山 、 牡 丹 江 、 哈 尔 滨 、 北 京 、 横 店 、 温 州 、 武 义 。 我 会 私 信 你 拿 地 址 。\r\n",
      "sentence label id is: 1\r\n",
      "---------------------------------\r\n",
      "2:\r\n",
      "sentence list id is: ['976', '1068', '1841', '2920', '3584', '1889', '2667', '2608', '3536', '527', '1418', '749', '1190', '2608', '1099', '1480', '2263', '2754', '527', '1190', '3961', '2507', '1192', '3975', '1554', '2211', '222', '749', '671', '3584', '1889', '4252', '4219', '1096', '352', '431', '1811', '2423', '1666', '314', '2979', '1498', '1482', '377', '1390', '4122', '1687', '1632', '1761', '4329', '491', '1733', '3880', '4345', '4392', '3168', '657', '4383', '920', '431', '1811', '2423', '1666', '314', '3880', '4345', '1482', '222', '749', '749', '3734', '2956', '450', '450', '749', '958', '1712', '2507', '450', '81', '1666', '1874', '4293', '3034', '2438', '314']\r\n",
      "sentence list is:  【 音 乐 · 预 告 】 # T a s t y # 新 曲 《 D a y ` n   N i g h t 》 预 告 影 像 释 出 ! [ x k l 眨 眼 ] 该 M V 在 香 港 拍 摄 ， 期 待 完 整 版 呦 ~ ! [ x k l 期 待 ] h t t p : / / t . c n / 8 k P B W R l\r\n",
      "sentence label id is: 1\r\n",
      "---------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "# 打印前2条训练数据\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "def ids_to_str(ids):\n",
    "    words = []\n",
    "    for k in ids:\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\n",
    "    return \" \".join(words)\n",
    "\n",
    "file_path = os.path.join(data_root_path, 'train.txt')\n",
    "with io.open(file_path, \"r\", encoding='utf8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            i += 1\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            if len(cols) != 2:\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                continue\n",
    "            label = int(cols[1])\n",
    "            wids = cols[0].split(\",\")\n",
    "            print(str(i)+\":\")\n",
    "            print('sentence list id is:', wids)\n",
    "            print('sentence list is: ', ids_to_str(wids))\n",
    "            print('sentence label id is:', label)\n",
    "            print('---------------------------------')\n",
    "            \n",
    "            if i == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.794738Z",
     "iopub.status.busy": "2024-01-05T12:26:45.785579Z",
     "iopub.status.idle": "2024-01-05T12:26:45.800715Z",
     "shell.execute_reply": "2024-01-05T12:26:45.799535Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.794686Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    fr = open(file_path, 'r', encoding='utf8')\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\n",
    "    fr.close()\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:45.802732Z",
     "iopub.status.busy": "2024-01-05T12:26:45.802177Z",
     "iopub.status.idle": "2024-01-05T12:26:46.155135Z",
     "shell.execute_reply": "2024-01-05T12:26:46.154260Z",
     "shell.execute_reply.started": "2024-01-05T12:26:45.802702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset\r\n",
      "[2608 1939 4288  707 4296 3433 2608 1192 1186  959 2156 4388 2773 2490\r\n",
      " 1152 3152  731 1009 2816 1036 1476 2836 4262  640 3221 1085 3378   56\r\n",
      "  298 3945 4031 1733 2309 4188  488 1852 4313 1351  577 4262  675  210\r\n",
      " 2773 1152   24 2156 1553 1361 1042  396 1733 2638 1941   24 1351 2156\r\n",
      "  406 1602 1234  121  769 2656 3380 1085 3378   56 3825 2838 3880  265\r\n",
      " 2156  788 2755  676 3583 3982 1245  915 1085  436  406  488 1676  298\r\n",
      "  675  210 3825 2341 4198 2448 1671 2374 2448 3381 2633 2448 3346 1226\r\n",
      " 2448 1338  254 2037 2448 1981 2577 2843 2448 1952 2770 2194 2448 3191\r\n",
      "  863 2448  361 2712 2448 1906 2374 2448 3381 1069 1085 1852 1152 3581\r\n",
      " 1553  396 3306  675 3385 1085 4410 4410 4410 4410 4410 4410 4410 4410\r\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410]\r\n",
      "(150,)\r\n",
      "[1]\r\n",
      "test_dataset\r\n",
      "[ 672 2288 1086 3120 2579  974 2410  519 1838 2263 3658 3982 1263 1568\r\n",
      "  671 3233 2504 3457  352 1733 3805  939 3805 1841 1045  284 3615 4188\r\n",
      " 3359 4091  820 3026 4304  629 3825 3745 3946 3889   94 3885  281 3874\r\n",
      " 3197 1467  741 2389 1733 3197 1467  741 3805  372  504 1549   55 2448\r\n",
      "  980 3925 1120 2097  903  140  249 1085  689 3613 1632 1761 2981 1687\r\n",
      " 1184 2093  298 3902 4091  820 3026 2085 1733 1632 1761 2316 4323 1618\r\n",
      " 2981 1687 3126 3390 3081 3424 1001 1085 1192 3317 2973 1911  352 1733\r\n",
      " 3902 3197 1467  741 2085 2293  470 1862 3885  281  577 2873 1733  420\r\n",
      "  504 3278 4252 1669  731 2638 1733 3805 2766  372 2926 3278 1549 2078\r\n",
      " 1733 1544 1152  167  372  140  249 1085 4410 4410 4410 4410 4410 4410\r\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410]\r\n",
      "(150,)\r\n",
      "[0]\r\n"
     ]
    }
   ],
   "source": [
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "class RumorDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_data = []\n",
    "       \n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\n",
    "            for line in fin:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                if len(cols) != 2:\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                    continue\n",
    "                label = []\n",
    "                label.append(int(cols[1]))\n",
    "                wids = cols[0].split(\",\")\n",
    "                if len(wids)>=150:\n",
    "                    wids = np.array(wids[:150]).astype('int64')     \n",
    "                else:\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]]*(150-len(wids))]).astype('int64')\n",
    "                label = np.array(label).astype('int64')\n",
    "                self.all_data.append((wids, label))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data, label = self.all_data[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = RumorDataset(os.path.join(data_root_path, 'train.txt'))\n",
    "test_dataset = RumorDataset(os.path.join(data_root_path, 'val.txt'))\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#检查\n",
    "\n",
    "print('train_dataset') \n",
    "for data, label in train_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n",
    "print('test_dataset') \n",
    "for data, label in test_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.157199Z",
     "iopub.status.busy": "2024-01-05T12:26:46.156317Z",
     "iopub.status.idle": "2024-01-05T12:26:46.165154Z",
     "shell.execute_reply": "2024-01-05T12:26:46.159980Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.157165Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 导入模型\n",
    "from model_utils import Transformer,CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.166977Z",
     "iopub.status.busy": "2024-01-05T12:26:46.166398Z",
     "iopub.status.idle": "2024-01-05T12:26:46.182420Z",
     "shell.execute_reply": "2024-01-05T12:26:46.180454Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.166944Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#定义卷积网络\n",
    "class Conv(nn.Layer):\n",
    "    def __init__(self,dict_dim):\n",
    "        super(Conv,self).__init__()\n",
    "        self.dict_dim = dict_dim\n",
    "        self.emb_dim = 128\n",
    "        self.hid_dim = 128\n",
    "        self.fc_hid_dim = 96\n",
    "        self.class_dim = 2\n",
    "        self.channels = 1\n",
    "        self.win_size = [3, self.hid_dim]\n",
    "        self.batch_size = 32\n",
    "        self.seq_len = 150\n",
    "        self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\n",
    "        self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\n",
    "                                            out_channels=self.hid_dim,        #卷积核个数\n",
    "                                            kernel_size=self.win_size,        #卷积核大小\n",
    "                                            padding=[1, 1]\n",
    "                                            )                         \n",
    "        self.relu1 = paddle.nn.ReLU()\n",
    "        self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\n",
    "                                            stride=2)             #池化步长2\n",
    "        self.hidden4 = paddle.nn.Linear(128*75, 2)\n",
    "    #网络的前向计算过程\n",
    "    def forward(self,input):\n",
    "        \n",
    "        #print('输入维度：', input.shape)\n",
    "        x = self.embedding(input)\n",
    "        x = paddle.reshape(x, [32, 1, 150, 128])   \n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        #print('第一层卷积输出维度：', x.shape)\n",
    "        x = self.hidden3(x)\n",
    "        #print('池化后输出维度：', x.shape)\n",
    "        #在输入全连接层时，需将特征图拉平会自动将数据拉平.\n",
    "\n",
    "        x = paddle.reshape(x, shape=[self.batch_size, -1])\n",
    "        out = self.hidden4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.183956Z",
     "iopub.status.busy": "2024-01-05T12:26:46.183625Z",
     "iopub.status.idle": "2024-01-05T12:26:46.193865Z",
     "shell.execute_reply": "2024-01-05T12:26:46.192321Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.183930Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vocab_size = len(vocab)  \n",
    "# maxlen = 200  \n",
    "# seq_len = 200\n",
    "# batch_size = 32\n",
    "# epochs = 100\n",
    "# pad_id = vocab['<pad>']\n",
    "# embed_dim = 128  # Embedding size for each token\n",
    "# num_heads = 2  # Number of attention heads\n",
    "# feed_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "# classes = ['0', '1']\n",
    "\n",
    "# transformer_model = Transformer(maxlen,vocab_size,embed_dim,num_heads,feed_dim)\n",
    "# paddle.summary(transformer_model,(200,128),\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.197457Z",
     "iopub.status.busy": "2024-01-05T12:26:46.197089Z",
     "iopub.status.idle": "2024-01-05T12:26:46.223967Z",
     "shell.execute_reply": "2024-01-05T12:26:46.222313Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.197418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\r\n",
      "        Layer (type)              Input Shape          Output Shape         Param #    \r\n",
      "=========================================================================================\r\n",
      "         Embedding-8                [[128]]             [128, 128]          25,600     \r\n",
      "         Embedding-7              [[200, 128]]       [200, 128, 128]        564,608    \r\n",
      " TokenAndPositionEmbedding-3      [[200, 128]]       [200, 128, 128]           0       \r\n",
      "          Linear-19            [[200, 128, 128]]     [200, 128, 128]        16,512     \r\n",
      "          Linear-20            [[200, 128, 128]]     [200, 128, 128]        16,512     \r\n",
      "          Linear-21            [[200, 128, 128]]     [200, 128, 128]        16,512     \r\n",
      "          Linear-22            [[200, 128, 128]]     [200, 128, 128]        16,512     \r\n",
      "  MultiHeadSelfAttention-3     [[200, 128, 128]]     [200, 128, 128]           0       \r\n",
      "          Dropout-9            [[200, 128, 128]]     [200, 128, 128]           0       \r\n",
      "         LayerNorm-5           [[200, 128, 128]]     [200, 128, 128]          256      \r\n",
      "          Linear-23            [[200, 128, 128]]     [200, 128, 128]        16,512     \r\n",
      "          Softmax-5            [[200, 128, 128]]     [200, 128, 128]           0       \r\n",
      "          Linear-24            [[200, 128, 128]]     [200, 128, 128]        16,512     \r\n",
      "PointWiseFeedForwardNetwork-3  [[200, 128, 128]]     [200, 128, 128]           0       \r\n",
      "         Dropout-10            [[200, 128, 128]]     [200, 128, 128]           0       \r\n",
      "         LayerNorm-6           [[200, 128, 128]]     [200, 128, 128]          256      \r\n",
      "     TransformerBlock-3        [[200, 128, 128]]     [200, 128, 128]           0       \r\n",
      "         Dropout-11               [[200, 128]]          [200, 128]             0       \r\n",
      "          Linear-25               [[200, 128]]          [200, 20]            2,580     \r\n",
      "           ReLU-5                 [[200, 20]]           [200, 20]              0       \r\n",
      "         Dropout-12               [[200, 20]]           [200, 20]              0       \r\n",
      "          Linear-26               [[200, 20]]            [200, 2]             42       \r\n",
      "          Softmax-6                [[200, 2]]            [200, 2]              0       \r\n",
      "=========================================================================================\r\n",
      "Total params: 692,414\r\n",
      "Trainable params: 692,414\r\n",
      "Non-trainable params: 0\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "Input size (MB): 0.10\r\n",
      "Forward/backward pass size (MB): 400.42\r\n",
      "Params size (MB): 2.64\r\n",
      "Estimated Total Size (MB): 403.16\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 692414, 'trainable_params': 692414}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)  \n",
    "maxlen = 200  \n",
    "seq_len = 200\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "pad_id = vocab['<pad>']\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "classes = ['0', '1']\n",
    "\n",
    "transformer_model = Transformer(maxlen,vocab_size,embed_dim,num_heads,feed_dim)\n",
    "paddle.summary(transformer_model,(200,128),\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.225974Z",
     "iopub.status.busy": "2024-01-05T12:26:46.225186Z",
     "iopub.status.idle": "2024-01-05T12:26:46.238752Z",
     "shell.execute_reply": "2024-01-05T12:26:46.237665Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.225910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs, model_name):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.0005, parameters=model.parameters())\n",
    "    \n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            \n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "            if batch_id % 50 == 0:\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy())\n",
    "                total_acc.append(acc.numpy())\n",
    "\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            \n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            \n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    paddle.save(model.state_dict(), model_name + \"_model_final.pdparams\")\n",
    "    \n",
    "    # draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    # draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.240137Z",
     "iopub.status.busy": "2024-01-05T12:26:46.239854Z",
     "iopub.status.idle": "2024-01-05T12:26:46.261073Z",
     "shell.execute_reply": "2024-01-05T12:26:46.252814Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.240111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\r\n",
      "  (emb): TokenAndPositionEmbedding(\r\n",
      "    (token_emb): Embedding(4411, 128, sparse=False)\r\n",
      "    (pos_emb): Embedding(200, 128, sparse=False)\r\n",
      "  )\r\n",
      "  (trs): TransformerBlock(\r\n",
      "    (att): MultiHeadSelfAttention(\r\n",
      "      (query_dense): Linear(in_features=128, out_features=128, dtype=float32)\r\n",
      "      (key_dense): Linear(in_features=128, out_features=128, dtype=float32)\r\n",
      "      (value_dense): Linear(in_features=128, out_features=128, dtype=float32)\r\n",
      "      (combine_heads): Linear(in_features=128, out_features=128, dtype=float32)\r\n",
      "    )\r\n",
      "    (ffn): PointWiseFeedForwardNetwork(\r\n",
      "      (linear1): Linear(in_features=128, out_features=128, dtype=float32)\r\n",
      "      (softmax): Softmax(axis=-1)\r\n",
      "      (linear2): Linear(in_features=128, out_features=128, dtype=float32)\r\n",
      "    )\r\n",
      "    (layernorm1): LayerNorm(normalized_shape=[128], epsilon=1e-06)\r\n",
      "    (layernorm2): LayerNorm(normalized_shape=[128], epsilon=1e-06)\r\n",
      "    (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "    (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "  )\r\n",
      "  (drop1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "  (linear1): Linear(in_features=128, out_features=20, dtype=float32)\r\n",
      "  (relu): ReLU()\r\n",
      "  (drop2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "  (linear2): Linear(in_features=20, out_features=2, dtype=float32)\r\n",
      "  (softmax): Softmax(axis=-1)\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:26:46.262629Z",
     "iopub.status.busy": "2024-01-05T12:26:46.262278Z",
     "iopub.status.idle": "2024-01-05T12:27:53.670726Z",
     "shell.execute_reply": "2024-01-05T12:27:53.669677Z",
     "shell.execute_reply.started": "2024-01-05T12:26:46.262600Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: 0.7064839601516724\r\n",
      "epoch: 0, batch_id: 50, loss is: 0.624210000038147\r\n",
      "[validation] accuracy: 0.7901785969734192, loss: 0.5301868915557861\r\n",
      "epoch: 1, batch_id: 0, loss is: 0.5464376211166382\r\n",
      "epoch: 1, batch_id: 50, loss is: 0.4608421325683594\r\n",
      "[validation] accuracy: 0.824404776096344, loss: 0.47930464148521423\r\n",
      "epoch: 2, batch_id: 0, loss is: 0.4322771430015564\r\n",
      "epoch: 2, batch_id: 50, loss is: 0.40623539686203003\r\n",
      "[validation] accuracy: 0.84375, loss: 0.4638700485229492\r\n",
      "epoch: 3, batch_id: 0, loss is: 0.35959360003471375\r\n",
      "epoch: 3, batch_id: 50, loss is: 0.3776213526725769\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.44555211067199707\r\n",
      "epoch: 4, batch_id: 0, loss is: 0.34895628690719604\r\n",
      "epoch: 4, batch_id: 50, loss is: 0.3528658151626587\r\n",
      "[validation] accuracy: 0.84375, loss: 0.45778506994247437\r\n",
      "epoch: 5, batch_id: 0, loss is: 0.353611022233963\r\n",
      "epoch: 5, batch_id: 50, loss is: 0.4396049976348877\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4472208619117737\r\n",
      "epoch: 6, batch_id: 0, loss is: 0.3438869118690491\r\n",
      "epoch: 6, batch_id: 50, loss is: 0.31720882654190063\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 0.45248186588287354\r\n",
      "epoch: 7, batch_id: 0, loss is: 0.3480347692966461\r\n",
      "epoch: 7, batch_id: 50, loss is: 0.31395992636680603\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 0.46338599920272827\r\n",
      "epoch: 8, batch_id: 0, loss is: 0.3135087192058563\r\n",
      "epoch: 8, batch_id: 50, loss is: 0.3447986841201782\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 0.45932456851005554\r\n",
      "epoch: 9, batch_id: 0, loss is: 0.31534522771835327\r\n",
      "epoch: 9, batch_id: 50, loss is: 0.3452203571796417\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.4560943841934204\r\n",
      "epoch: 10, batch_id: 0, loss is: 0.31329941749572754\r\n",
      "epoch: 10, batch_id: 50, loss is: 0.313740611076355\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 0.4619190990924835\r\n",
      "epoch: 11, batch_id: 0, loss is: 0.31331658363342285\r\n",
      "epoch: 11, batch_id: 50, loss is: 0.32746201753616333\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.4503929018974304\r\n",
      "epoch: 12, batch_id: 0, loss is: 0.3486042022705078\r\n",
      "epoch: 12, batch_id: 50, loss is: 0.31567078828811646\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.45103734731674194\r\n",
      "epoch: 13, batch_id: 0, loss is: 0.3135860562324524\r\n",
      "epoch: 13, batch_id: 50, loss is: 0.344429075717926\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4471466839313507\r\n",
      "epoch: 14, batch_id: 0, loss is: 0.34451472759246826\r\n",
      "epoch: 14, batch_id: 50, loss is: 0.3133543133735657\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.45316392183303833\r\n",
      "epoch: 15, batch_id: 0, loss is: 0.3449235260486603\r\n",
      "epoch: 15, batch_id: 50, loss is: 0.31328824162483215\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.451487272977829\r\n",
      "epoch: 16, batch_id: 0, loss is: 0.34480956196784973\r\n",
      "epoch: 16, batch_id: 50, loss is: 0.3132742643356323\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.44995665550231934\r\n",
      "epoch: 17, batch_id: 0, loss is: 0.31328487396240234\r\n",
      "epoch: 17, batch_id: 50, loss is: 0.3445134162902832\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4428619146347046\r\n",
      "epoch: 18, batch_id: 0, loss is: 0.34451231360435486\r\n",
      "epoch: 18, batch_id: 50, loss is: 0.3797326683998108\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.4530685544013977\r\n",
      "epoch: 19, batch_id: 0, loss is: 0.40701285004615784\r\n",
      "epoch: 19, batch_id: 50, loss is: 0.31369706988334656\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.4512765407562256\r\n",
      "epoch: 20, batch_id: 0, loss is: 0.3132655620574951\r\n",
      "epoch: 20, batch_id: 50, loss is: 0.3133038282394409\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4481181204319\r\n",
      "epoch: 21, batch_id: 0, loss is: 0.3759201169013977\r\n",
      "epoch: 21, batch_id: 50, loss is: 0.344609797000885\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4459204077720642\r\n",
      "epoch: 22, batch_id: 0, loss is: 0.3132645785808563\r\n",
      "epoch: 22, batch_id: 50, loss is: 0.3757619261741638\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44527631998062134\r\n",
      "epoch: 23, batch_id: 0, loss is: 0.3132672905921936\r\n",
      "epoch: 23, batch_id: 50, loss is: 0.34451180696487427\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4506109356880188\r\n",
      "epoch: 24, batch_id: 0, loss is: 0.3135603070259094\r\n",
      "epoch: 24, batch_id: 50, loss is: 0.313345730304718\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.4527283310890198\r\n",
      "epoch: 25, batch_id: 0, loss is: 0.3132782578468323\r\n",
      "epoch: 25, batch_id: 50, loss is: 0.3141341209411621\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.4544767737388611\r\n",
      "epoch: 26, batch_id: 0, loss is: 0.3135230243206024\r\n",
      "epoch: 26, batch_id: 50, loss is: 0.31326425075531006\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.4528360366821289\r\n",
      "epoch: 27, batch_id: 0, loss is: 0.3592415153980255\r\n",
      "epoch: 27, batch_id: 50, loss is: 0.3133983016014099\r\n",
      "[validation] accuracy: 0.8541666865348816, loss: 0.45798060297966003\r\n",
      "epoch: 28, batch_id: 0, loss is: 0.3132627606391907\r\n",
      "epoch: 28, batch_id: 50, loss is: 0.3445169925689697\r\n",
      "[validation] accuracy: 0.8526785969734192, loss: 0.4582173526287079\r\n",
      "epoch: 29, batch_id: 0, loss is: 0.31327080726623535\r\n",
      "epoch: 29, batch_id: 50, loss is: 0.31326356530189514\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 0.4645434319972992\r\n",
      "epoch: 30, batch_id: 0, loss is: 0.313551127910614\r\n",
      "epoch: 30, batch_id: 50, loss is: 0.31326234340667725\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.45988109707832336\r\n",
      "epoch: 31, batch_id: 0, loss is: 0.3133217692375183\r\n",
      "epoch: 31, batch_id: 50, loss is: 0.3133578300476074\r\n",
      "[validation] accuracy: 0.824404776096344, loss: 0.4883939325809479\r\n",
      "epoch: 32, batch_id: 0, loss is: 0.3617318570613861\r\n",
      "epoch: 32, batch_id: 50, loss is: 0.3133036494255066\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.45105111598968506\r\n",
      "epoch: 33, batch_id: 0, loss is: 0.3136168122291565\r\n",
      "epoch: 33, batch_id: 50, loss is: 0.31337642669677734\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 0.4597567617893219\r\n",
      "epoch: 34, batch_id: 0, loss is: 0.4070284366607666\r\n",
      "epoch: 34, batch_id: 50, loss is: 0.32112419605255127\r\n",
      "[validation] accuracy: 0.855654776096344, loss: 0.456924170255661\r\n",
      "epoch: 35, batch_id: 0, loss is: 0.3132619261741638\r\n",
      "epoch: 35, batch_id: 50, loss is: 0.3132919371128082\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4450809955596924\r\n",
      "epoch: 36, batch_id: 0, loss is: 0.34451860189437866\r\n",
      "epoch: 36, batch_id: 50, loss is: 0.31328481435775757\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4485263228416443\r\n",
      "epoch: 37, batch_id: 0, loss is: 0.313270628452301\r\n",
      "epoch: 37, batch_id: 50, loss is: 0.31326234340667725\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44676461815834045\r\n",
      "epoch: 38, batch_id: 0, loss is: 0.3132636249065399\r\n",
      "epoch: 38, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.4497250020503998\r\n",
      "epoch: 39, batch_id: 0, loss is: 0.3132625222206116\r\n",
      "epoch: 39, batch_id: 50, loss is: 0.3132624626159668\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.44874632358551025\r\n",
      "epoch: 40, batch_id: 0, loss is: 0.3132644295692444\r\n",
      "epoch: 40, batch_id: 50, loss is: 0.3757629990577698\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.4539302587509155\r\n",
      "epoch: 41, batch_id: 0, loss is: 0.31329482793807983\r\n",
      "epoch: 41, batch_id: 50, loss is: 0.344512403011322\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.45081621408462524\r\n",
      "epoch: 42, batch_id: 0, loss is: 0.31326180696487427\r\n",
      "epoch: 42, batch_id: 50, loss is: 0.3132621645927429\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.449552446603775\r\n",
      "epoch: 43, batch_id: 0, loss is: 0.34451213479042053\r\n",
      "epoch: 43, batch_id: 50, loss is: 0.31327366828918457\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.448579877614975\r\n",
      "epoch: 44, batch_id: 0, loss is: 0.31326186656951904\r\n",
      "epoch: 44, batch_id: 50, loss is: 0.3445129990577698\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4480595290660858\r\n",
      "epoch: 45, batch_id: 0, loss is: 0.31326210498809814\r\n",
      "epoch: 45, batch_id: 50, loss is: 0.31326454877853394\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.44954636693000793\r\n",
      "epoch: 46, batch_id: 0, loss is: 0.31326499581336975\r\n",
      "epoch: 46, batch_id: 50, loss is: 0.31326234340667725\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.44963327050209045\r\n",
      "epoch: 47, batch_id: 0, loss is: 0.31326204538345337\r\n",
      "epoch: 47, batch_id: 50, loss is: 0.3445122539997101\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.44952192902565\r\n",
      "epoch: 48, batch_id: 0, loss is: 0.31326207518577576\r\n",
      "epoch: 48, batch_id: 50, loss is: 0.3132641315460205\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.4495517313480377\r\n",
      "epoch: 49, batch_id: 0, loss is: 0.31326210498809814\r\n",
      "epoch: 49, batch_id: 50, loss is: 0.313263475894928\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4463787078857422\r\n",
      "epoch: 50, batch_id: 0, loss is: 0.31331509351730347\r\n",
      "epoch: 50, batch_id: 50, loss is: 0.3132985830307007\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.447421669960022\r\n",
      "epoch: 51, batch_id: 0, loss is: 0.3445122241973877\r\n",
      "epoch: 51, batch_id: 50, loss is: 0.31326353549957275\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.44810131192207336\r\n",
      "epoch: 52, batch_id: 0, loss is: 0.34451431035995483\r\n",
      "epoch: 52, batch_id: 50, loss is: 0.3132617771625519\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44662269949913025\r\n",
      "epoch: 53, batch_id: 0, loss is: 0.31326207518577576\r\n",
      "epoch: 53, batch_id: 50, loss is: 0.31326189637184143\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4481339752674103\r\n",
      "epoch: 54, batch_id: 0, loss is: 0.3132617771625519\r\n",
      "epoch: 54, batch_id: 50, loss is: 0.37576180696487427\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44793325662612915\r\n",
      "epoch: 55, batch_id: 0, loss is: 0.3445146083831787\r\n",
      "epoch: 55, batch_id: 50, loss is: 0.3132675290107727\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4481748342514038\r\n",
      "epoch: 56, batch_id: 0, loss is: 0.31326204538345337\r\n",
      "epoch: 56, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.4498024880886078\r\n",
      "epoch: 57, batch_id: 0, loss is: 0.34451282024383545\r\n",
      "epoch: 57, batch_id: 50, loss is: 0.3132690191268921\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.4498623013496399\r\n",
      "epoch: 58, batch_id: 0, loss is: 0.3132619559764862\r\n",
      "epoch: 58, batch_id: 50, loss is: 0.3445124626159668\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4462320804595947\r\n",
      "epoch: 59, batch_id: 0, loss is: 0.31326207518577576\r\n",
      "epoch: 59, batch_id: 50, loss is: 0.31326228380203247\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4483179748058319\r\n",
      "epoch: 60, batch_id: 0, loss is: 0.31326383352279663\r\n",
      "epoch: 60, batch_id: 50, loss is: 0.31326404213905334\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4462519586086273\r\n",
      "epoch: 61, batch_id: 0, loss is: 0.3445131182670593\r\n",
      "epoch: 61, batch_id: 50, loss is: 0.3132627606391907\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.44610437750816345\r\n",
      "epoch: 62, batch_id: 0, loss is: 0.34451359510421753\r\n",
      "epoch: 62, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4492332339286804\r\n",
      "epoch: 63, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 63, batch_id: 50, loss is: 0.34451180696487427\r\n",
      "[validation] accuracy: 0.8690476417541504, loss: 0.44457194209098816\r\n",
      "epoch: 64, batch_id: 0, loss is: 0.3132619857788086\r\n",
      "epoch: 64, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4490821063518524\r\n",
      "epoch: 65, batch_id: 0, loss is: 0.3132621645927429\r\n",
      "epoch: 65, batch_id: 50, loss is: 0.3132626712322235\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4476465880870819\r\n",
      "epoch: 66, batch_id: 0, loss is: 0.37576180696487427\r\n",
      "epoch: 66, batch_id: 50, loss is: 0.3132617771625519\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44761544466018677\r\n",
      "epoch: 67, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 67, batch_id: 50, loss is: 0.3132632076740265\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.44927677512168884\r\n",
      "epoch: 68, batch_id: 0, loss is: 0.3445141315460205\r\n",
      "epoch: 68, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44784826040267944\r\n",
      "epoch: 69, batch_id: 0, loss is: 0.31326210498809814\r\n",
      "epoch: 69, batch_id: 50, loss is: 0.31326305866241455\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4479841887950897\r\n",
      "epoch: 70, batch_id: 0, loss is: 0.31326210498809814\r\n",
      "epoch: 70, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.44922715425491333\r\n",
      "epoch: 71, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 71, batch_id: 50, loss is: 0.3445166349411011\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44757789373397827\r\n",
      "epoch: 72, batch_id: 0, loss is: 0.31326183676719666\r\n",
      "epoch: 72, batch_id: 50, loss is: 0.34451183676719666\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4474199414253235\r\n",
      "epoch: 73, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 73, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.449267715215683\r\n",
      "epoch: 74, batch_id: 0, loss is: 0.3132632374763489\r\n",
      "epoch: 74, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.44745126366615295\r\n",
      "epoch: 75, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 75, batch_id: 50, loss is: 0.31326213479042053\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4490136504173279\r\n",
      "epoch: 76, batch_id: 0, loss is: 0.34451213479042053\r\n",
      "epoch: 76, batch_id: 50, loss is: 0.31326180696487427\r\n",
      "[validation] accuracy: 0.8690476417541504, loss: 0.4460887312889099\r\n",
      "epoch: 77, batch_id: 0, loss is: 0.31326189637184143\r\n",
      "epoch: 77, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.44901061058044434\r\n",
      "epoch: 78, batch_id: 0, loss is: 0.3132619261741638\r\n",
      "epoch: 78, batch_id: 50, loss is: 0.31326210498809814\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4476057291030884\r\n",
      "epoch: 79, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 79, batch_id: 50, loss is: 0.3757617771625519\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4490599036216736\r\n",
      "epoch: 80, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 80, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.44881942868232727\r\n",
      "epoch: 81, batch_id: 0, loss is: 0.34451285004615784\r\n",
      "epoch: 81, batch_id: 50, loss is: 0.31326189637184143\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.4482330083847046\r\n",
      "epoch: 82, batch_id: 0, loss is: 0.3445117771625519\r\n",
      "epoch: 82, batch_id: 50, loss is: 0.31326180696487427\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4478034973144531\r\n",
      "epoch: 83, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 83, batch_id: 50, loss is: 0.3132619261741638\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4490240216255188\r\n",
      "epoch: 84, batch_id: 0, loss is: 0.3132617771625519\r\n",
      "epoch: 84, batch_id: 50, loss is: 0.34451180696487427\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4474179446697235\r\n",
      "epoch: 85, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 85, batch_id: 50, loss is: 0.34451180696487427\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4474216103553772\r\n",
      "epoch: 86, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 86, batch_id: 50, loss is: 0.31326228380203247\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4489121735095978\r\n",
      "epoch: 87, batch_id: 0, loss is: 0.31326183676719666\r\n",
      "epoch: 87, batch_id: 50, loss is: 0.34451186656951904\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4473888874053955\r\n",
      "epoch: 88, batch_id: 0, loss is: 0.3445117473602295\r\n",
      "epoch: 88, batch_id: 50, loss is: 0.3132619261741638\r\n",
      "[validation] accuracy: 0.8690476417541504, loss: 0.44609683752059937\r\n",
      "epoch: 89, batch_id: 0, loss is: 0.31326180696487427\r\n",
      "epoch: 89, batch_id: 50, loss is: 0.31326180696487427\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.449363648891449\r\n",
      "epoch: 90, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 90, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.44882452487945557\r\n",
      "epoch: 91, batch_id: 0, loss is: 0.31326180696487427\r\n",
      "epoch: 91, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.44905656576156616\r\n",
      "epoch: 92, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 92, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.4475950002670288\r\n",
      "epoch: 93, batch_id: 0, loss is: 0.3445119261741638\r\n",
      "epoch: 93, batch_id: 50, loss is: 0.31326180696487427\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.44713255763053894\r\n",
      "epoch: 94, batch_id: 0, loss is: 0.3132617771625519\r\n",
      "epoch: 94, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.867559552192688, loss: 0.44757020473480225\r\n",
      "epoch: 95, batch_id: 0, loss is: 0.34451180696487427\r\n",
      "epoch: 95, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.863095223903656, loss: 0.4497576355934143\r\n",
      "epoch: 96, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 96, batch_id: 50, loss is: 0.31326183676719666\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.4480392038822174\r\n",
      "epoch: 97, batch_id: 0, loss is: 0.31344133615493774\r\n",
      "epoch: 97, batch_id: 50, loss is: 0.3445117473602295\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.45070162415504456\r\n",
      "epoch: 98, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 98, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.4516426622867584\r\n",
      "epoch: 99, batch_id: 0, loss is: 0.3132617473602295\r\n",
      "epoch: 99, batch_id: 50, loss is: 0.3132617473602295\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.45106056332588196\r\n",
      "running_time is 67.40290570259094\r\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "train(transformer_model, epochs, \"transformer\")\n",
    "end_time=time.time()\n",
    "running_time=end_time-start_time\n",
    "print(\"running_time is {}\".format(running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:27:53.672368Z",
     "iopub.status.busy": "2024-01-05T12:27:53.671951Z",
     "iopub.status.idle": "2024-01-05T12:27:53.761070Z",
     "shell.execute_reply": "2024-01-05T12:27:53.760197Z",
     "shell.execute_reply.started": "2024-01-05T12:27:53.672337Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] accuracy: 0.8645833134651184, loss: 0.44808441400527954\r\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = paddle.load('transformer_model_final.pdparams')\n",
    "transformer_model.set_state_dict(model_state_dict) \n",
    "transformer_model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    logits = transformer_model(sent)\n",
    "    loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "    acc = paddle.metric.accuracy(logits, label)\n",
    "    \n",
    "    accuracies.append(acc.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:27:53.762533Z",
     "iopub.status.busy": "2024-01-05T12:27:53.762168Z",
     "iopub.status.idle": "2024-01-05T12:27:53.930754Z",
     "shell.execute_reply": "2024-01-05T12:27:53.929981Z",
     "shell.execute_reply.started": "2024-01-05T12:27:53.762507Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.8616,pre:0.8646,rec:0.8671,f1:0.8658\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "           0     0.8585    0.8558    0.8571       326\r\n",
      "           1     0.8646    0.8671    0.8658       346\r\n",
      "\r\n",
      "    accuracy                         0.8616       672\r\n",
      "   macro avg     0.8615    0.8614    0.8615       672\r\n",
      "weighted avg     0.8616    0.8616    0.8616       672\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for emb.token_emb.weight. emb.token_emb.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for emb.pos_emb.weight. emb.pos_emb.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.query_dense.weight. trs.att.query_dense.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.query_dense.bias. trs.att.query_dense.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.key_dense.weight. trs.att.key_dense.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.key_dense.bias. trs.att.key_dense.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.value_dense.weight. trs.att.value_dense.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.value_dense.bias. trs.att.value_dense.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.combine_heads.weight. trs.att.combine_heads.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.combine_heads.bias. trs.att.combine_heads.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear1.weight. trs.ffn.linear1.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear1.bias. trs.ffn.linear1.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear2.weight. trs.ffn.linear2.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear2.bias. trs.ffn.linear2.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm1.weight. trs.layernorm1.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm1.bias. trs.layernorm1.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm2.weight. trs.layernorm2.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm2.bias. trs.layernorm2.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear1.weight. linear1.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear1.bias. linear1.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear2.weight. linear2.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear2.bias. linear2.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "model_state_dict = paddle.load('model_final.pdparams')\n",
    "transformer_model.set_state_dict(model_state_dict) \n",
    "transformer_model.eval()\n",
    "predictions = []\n",
    "r = []\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    sent = data[0]\n",
    "    gt_labels = data[1].numpy()\n",
    "    for i in gt_labels:\n",
    "        r.append(i)\n",
    "    results = transformer_model(sent)\n",
    "    for probs in results:\n",
    "        # 映射分类label\n",
    "        idx = np.argmax(probs)\n",
    "        predictions.append(idx)\n",
    "    \n",
    "confusion_matrix(r, predictions)\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"0\",\"1\"]\n",
    "acc = accuracy_score(r, predictions).round(4)\n",
    "pre = precision_score(r, predictions).round(4)\n",
    "rec = recall_score(r, predictions).round(4)\n",
    "F1 = f1_score(r, predictions).round(4)\n",
    "print('acc:{},pre:{},rec:{},f1:{}'.format(acc,pre,rec,F1))\n",
    "CR=classification_report(r, predictions, target_names=target_names,digits=4)\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:27:53.932279Z",
     "iopub.status.busy": "2024-01-05T12:27:53.931891Z",
     "iopub.status.idle": "2024-01-05T12:27:54.021377Z",
     "shell.execute_reply": "2024-01-05T12:27:54.020535Z",
     "shell.execute_reply.started": "2024-01-05T12:27:53.932249Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for emb.token_emb.weight. emb.token_emb.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for emb.pos_emb.weight. emb.pos_emb.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.query_dense.weight. trs.att.query_dense.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.query_dense.bias. trs.att.query_dense.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.key_dense.weight. trs.att.key_dense.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.key_dense.bias. trs.att.key_dense.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.value_dense.weight. trs.att.value_dense.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.value_dense.bias. trs.att.value_dense.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.combine_heads.weight. trs.att.combine_heads.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.att.combine_heads.bias. trs.att.combine_heads.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear1.weight. trs.ffn.linear1.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear1.bias. trs.ffn.linear1.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear2.weight. trs.ffn.linear2.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.ffn.linear2.bias. trs.ffn.linear2.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm1.weight. trs.layernorm1.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm1.bias. trs.layernorm1.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm2.weight. trs.layernorm2.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for trs.layernorm2.bias. trs.layernorm2.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear1.weight. linear1.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear1.bias. linear1.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear2.weight. linear2.weight is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.10/site-packages/paddle/nn/layer/layers.py:2084: UserWarning: Skip loading for linear2.bias. linear2.bias is not found in the provided dict.\r\n",
      "  warnings.warn(f\"Skip loading for {key}. \" + str(err))\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据: #熊先生一周岁#我的熊宝宝在大家的关爱下已经马上要一岁了，这一年里面是大家一把屎一把尿把我的熊宝宝喂养大的，我们也会在熊先生的生日月带来各种活动，有跟跨界品牌之间的互动合作，有跟艺人之间的定制款，有跟潮流牌品的限定定制款，还有一系列回馈老顾主的活动，感谢熊粉！熊粉万岁！@熊先生品牌 \r\n",
      "\r\n",
      "预测: 不是谣言 \r\n",
      "原始标签：不是谣言\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_map = {0:\"谣言\", 1:\"不是谣言\"}\n",
    "\n",
    "model_state_dict = paddle.load('model_final.pdparams')\n",
    "transformer_model.set_state_dict(model_state_dict) \n",
    "transformer_model.eval()\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    gt_labels = data[1].numpy()\n",
    "   \n",
    "    results = transformer_model(sent)\n",
    "\n",
    "    predictions = []\n",
    "    for probs in results:\n",
    "        # 映射分类label\n",
    "        idx = np.argmax(probs)\n",
    "        labels = label_map[idx]\n",
    "        predictions.append(labels)\n",
    "    \n",
    "    for i,pre in enumerate(predictions):\n",
    "        print('数据: {} \\n\\n预测: {} \\n原始标签：{}'.format(ids_to_str(sent[0]).replace(\" \", \"\").replace(\"<pad>\",\"\"), pre, label_map[gt_labels[0][0]]))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T12:27:54.023114Z",
     "iopub.status.busy": "2024-01-05T12:27:54.022584Z",
     "iopub.status.idle": "2024-01-05T12:28:34.848040Z",
     "shell.execute_reply": "2024-01-05T12:28:34.846987Z",
     "shell.execute_reply.started": "2024-01-05T12:27:54.023083Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: 0.6830382347106934\r\n",
      "epoch: 0, batch_id: 50, loss is: 0.6812448501586914\r\n",
      "[validation] accuracy: 0.5565476417541504, loss: 0.6595727801322937\r\n",
      "epoch: 1, batch_id: 0, loss is: 0.6040115356445312\r\n",
      "epoch: 1, batch_id: 50, loss is: 0.4054224491119385\r\n",
      "[validation] accuracy: 0.8407738208770752, loss: 0.39349228143692017\r\n",
      "epoch: 2, batch_id: 0, loss is: 0.13129544258117676\r\n",
      "epoch: 2, batch_id: 50, loss is: 0.2221955955028534\r\n",
      "[validation] accuracy: 0.8407738208770752, loss: 0.36038738489151\r\n",
      "epoch: 3, batch_id: 0, loss is: 0.11491723358631134\r\n",
      "epoch: 3, batch_id: 50, loss is: 0.14197418093681335\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.35448211431503296\r\n",
      "epoch: 4, batch_id: 0, loss is: 0.08386333286762238\r\n",
      "epoch: 4, batch_id: 50, loss is: 0.02135905623435974\r\n",
      "[validation] accuracy: 0.855654776096344, loss: 0.4239746332168579\r\n",
      "epoch: 5, batch_id: 0, loss is: 0.03588009625673294\r\n",
      "epoch: 5, batch_id: 50, loss is: 0.025930725038051605\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.44237130880355835\r\n",
      "epoch: 6, batch_id: 0, loss is: 0.00984046421945095\r\n",
      "epoch: 6, batch_id: 50, loss is: 0.016659844666719437\r\n",
      "[validation] accuracy: 0.8645833134651184, loss: 0.47690314054489136\r\n",
      "epoch: 7, batch_id: 0, loss is: 0.005760728847235441\r\n",
      "epoch: 7, batch_id: 50, loss is: 0.02632407657802105\r\n",
      "[validation] accuracy: 0.8660714030265808, loss: 0.5204871892929077\r\n",
      "epoch: 8, batch_id: 0, loss is: 0.010407701134681702\r\n",
      "epoch: 8, batch_id: 50, loss is: 0.005681974347680807\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.5702947974205017\r\n",
      "epoch: 9, batch_id: 0, loss is: 0.006669436581432819\r\n",
      "epoch: 9, batch_id: 50, loss is: 0.003088179510086775\r\n",
      "[validation] accuracy: 0.8616071343421936, loss: 0.618148684501648\r\n",
      "epoch: 10, batch_id: 0, loss is: 0.008353600278496742\r\n",
      "epoch: 10, batch_id: 50, loss is: 0.005851300433278084\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.6441842913627625\r\n",
      "epoch: 11, batch_id: 0, loss is: 0.001868829713203013\r\n",
      "epoch: 11, batch_id: 50, loss is: 0.002594303572550416\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.6502035856246948\r\n",
      "epoch: 12, batch_id: 0, loss is: 0.0008741678320802748\r\n",
      "epoch: 12, batch_id: 50, loss is: 0.0014685760252177715\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.6922780871391296\r\n",
      "epoch: 13, batch_id: 0, loss is: 0.0014856276102364063\r\n",
      "epoch: 13, batch_id: 50, loss is: 0.00216495874337852\r\n",
      "[validation] accuracy: 0.8526785969734192, loss: 0.717627227306366\r\n",
      "epoch: 14, batch_id: 0, loss is: 0.0009198131738230586\r\n",
      "epoch: 14, batch_id: 50, loss is: 0.0013808663934469223\r\n",
      "[validation] accuracy: 0.8541666865348816, loss: 0.7387099862098694\r\n",
      "epoch: 15, batch_id: 0, loss is: 0.0006846020696684718\r\n",
      "epoch: 15, batch_id: 50, loss is: 0.0008209902443923056\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.7488009333610535\r\n",
      "epoch: 16, batch_id: 0, loss is: 0.0006747549632564187\r\n",
      "epoch: 16, batch_id: 50, loss is: 0.0011913140770047903\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.7760237455368042\r\n",
      "epoch: 17, batch_id: 0, loss is: 0.0002094290975946933\r\n",
      "epoch: 17, batch_id: 50, loss is: 0.001590742263942957\r\n",
      "[validation] accuracy: 0.8601190447807312, loss: 0.7867530584335327\r\n",
      "epoch: 18, batch_id: 0, loss is: 0.0014892974868416786\r\n",
      "epoch: 18, batch_id: 50, loss is: 0.0004734178655780852\r\n",
      "[validation] accuracy: 0.8541666865348816, loss: 0.8111323118209839\r\n",
      "epoch: 19, batch_id: 0, loss is: 0.0003543419297784567\r\n",
      "epoch: 19, batch_id: 50, loss is: 0.0008941808482632041\r\n",
      "[validation] accuracy: 0.855654776096344, loss: 0.8153195381164551\r\n",
      "epoch: 20, batch_id: 0, loss is: 0.00023296131985262036\r\n",
      "epoch: 20, batch_id: 50, loss is: 0.0008657344151288271\r\n",
      "[validation] accuracy: 0.8586309552192688, loss: 0.8389836549758911\r\n",
      "epoch: 21, batch_id: 0, loss is: 0.0005282168858684599\r\n",
      "epoch: 21, batch_id: 50, loss is: 0.00039489445043727756\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.8382565379142761\r\n",
      "epoch: 22, batch_id: 0, loss is: 0.0002935557859018445\r\n",
      "epoch: 22, batch_id: 50, loss is: 0.0005076306406408548\r\n",
      "[validation] accuracy: 0.8571428656578064, loss: 0.8635295033454895\r\n",
      "epoch: 23, batch_id: 0, loss is: 0.0005130262579768896\r\n",
      "epoch: 23, batch_id: 50, loss is: 0.000470332452096045\r\n",
      "[validation] accuracy: 0.8526785969734192, loss: 0.878842294216156\r\n",
      "epoch: 24, batch_id: 0, loss is: 0.00020332801796030253\r\n",
      "epoch: 24, batch_id: 50, loss is: 0.00010428058885736391\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.8973537683486938\r\n",
      "epoch: 25, batch_id: 0, loss is: 0.00033163547050207853\r\n",
      "epoch: 25, batch_id: 50, loss is: 0.0001340558083029464\r\n",
      "[validation] accuracy: 0.8541666865348816, loss: 0.9062319993972778\r\n",
      "epoch: 26, batch_id: 0, loss is: 0.00015299169172067195\r\n",
      "epoch: 26, batch_id: 50, loss is: 0.0002032123156823218\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.9212294220924377\r\n",
      "epoch: 27, batch_id: 0, loss is: 0.0001341665629297495\r\n",
      "epoch: 27, batch_id: 50, loss is: 0.0003940619935747236\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 0.9340147972106934\r\n",
      "epoch: 28, batch_id: 0, loss is: 0.00022311534848995507\r\n",
      "epoch: 28, batch_id: 50, loss is: 0.00039342668605968356\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.945257306098938\r\n",
      "epoch: 29, batch_id: 0, loss is: 0.00017105345614254475\r\n",
      "epoch: 29, batch_id: 50, loss is: 3.3497541153337806e-05\r\n",
      "[validation] accuracy: 0.8526785969734192, loss: 0.9430211186408997\r\n",
      "epoch: 30, batch_id: 0, loss is: 9.557518205838278e-05\r\n",
      "epoch: 30, batch_id: 50, loss is: 4.965946573065594e-05\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.9599871635437012\r\n",
      "epoch: 31, batch_id: 0, loss is: 0.00018758262740448117\r\n",
      "epoch: 31, batch_id: 50, loss is: 9.99228359432891e-05\r\n",
      "[validation] accuracy: 0.855654776096344, loss: 0.9462372064590454\r\n",
      "epoch: 32, batch_id: 0, loss is: 6.34088137303479e-05\r\n",
      "epoch: 32, batch_id: 50, loss is: 0.00013726727047469467\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.98194420337677\r\n",
      "epoch: 33, batch_id: 0, loss is: 0.00021866144379600883\r\n",
      "epoch: 33, batch_id: 50, loss is: 7.985737465787679e-05\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 0.9921727776527405\r\n",
      "epoch: 34, batch_id: 0, loss is: 7.748010102659464e-05\r\n",
      "epoch: 34, batch_id: 50, loss is: 8.17499530967325e-05\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 1.0074092149734497\r\n",
      "epoch: 35, batch_id: 0, loss is: 0.0001286368496948853\r\n",
      "epoch: 35, batch_id: 50, loss is: 0.00010032746649812907\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.0175158977508545\r\n",
      "epoch: 36, batch_id: 0, loss is: 6.548644159920514e-05\r\n",
      "epoch: 36, batch_id: 50, loss is: 6.598610343644395e-05\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 0.9851802587509155\r\n",
      "epoch: 37, batch_id: 0, loss is: 0.00021991695393808186\r\n",
      "epoch: 37, batch_id: 50, loss is: 7.876026211306453e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.0251790285110474\r\n",
      "epoch: 38, batch_id: 0, loss is: 7.548494613729417e-05\r\n",
      "epoch: 38, batch_id: 50, loss is: 7.14225388946943e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.0464216470718384\r\n",
      "epoch: 39, batch_id: 0, loss is: 6.597379251616076e-05\r\n",
      "epoch: 39, batch_id: 50, loss is: 0.0001053574524121359\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 1.0349340438842773\r\n",
      "epoch: 40, batch_id: 0, loss is: 6.850411591585726e-05\r\n",
      "epoch: 40, batch_id: 50, loss is: 5.562961450777948e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.0352152585983276\r\n",
      "epoch: 41, batch_id: 0, loss is: 5.571961810346693e-05\r\n",
      "epoch: 41, batch_id: 50, loss is: 6.1005841416772455e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.035875678062439\r\n",
      "epoch: 42, batch_id: 0, loss is: 8.614074613433331e-05\r\n",
      "epoch: 42, batch_id: 50, loss is: 3.198618651367724e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.0821635723114014\r\n",
      "epoch: 43, batch_id: 0, loss is: 6.350516923703253e-05\r\n",
      "epoch: 43, batch_id: 50, loss is: 5.204334229347296e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.090070366859436\r\n",
      "epoch: 44, batch_id: 0, loss is: 3.692065365612507e-05\r\n",
      "epoch: 44, batch_id: 50, loss is: 4.28463208663743e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.0871033668518066\r\n",
      "epoch: 45, batch_id: 0, loss is: 4.565395647659898e-05\r\n",
      "epoch: 45, batch_id: 50, loss is: 2.1788038793602027e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.0835771560668945\r\n",
      "epoch: 46, batch_id: 0, loss is: 3.260132507421076e-05\r\n",
      "epoch: 46, batch_id: 50, loss is: 3.4269374737050384e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.11655855178833\r\n",
      "epoch: 47, batch_id: 0, loss is: 2.7516522095538676e-05\r\n",
      "epoch: 47, batch_id: 50, loss is: 1.3939432392362505e-05\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 1.1199091672897339\r\n",
      "epoch: 48, batch_id: 0, loss is: 2.7128389774588868e-05\r\n",
      "epoch: 48, batch_id: 50, loss is: 2.5235152861569077e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.133406400680542\r\n",
      "epoch: 49, batch_id: 0, loss is: 2.587801282061264e-05\r\n",
      "epoch: 49, batch_id: 50, loss is: 1.831994450185448e-05\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.1419686079025269\r\n",
      "epoch: 50, batch_id: 0, loss is: 3.335395740577951e-05\r\n",
      "epoch: 50, batch_id: 50, loss is: 1.9851166143780574e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.1496095657348633\r\n",
      "epoch: 51, batch_id: 0, loss is: 2.6357836759416386e-05\r\n",
      "epoch: 51, batch_id: 50, loss is: 3.236616248614155e-05\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 1.151230812072754\r\n",
      "epoch: 52, batch_id: 0, loss is: 4.283603630028665e-05\r\n",
      "epoch: 52, batch_id: 50, loss is: 8.451638859696686e-05\r\n",
      "[validation] accuracy: 0.851190447807312, loss: 1.142748475074768\r\n",
      "epoch: 53, batch_id: 0, loss is: 2.5725117666297592e-05\r\n",
      "epoch: 53, batch_id: 50, loss is: 2.7462190701044165e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.1730695962905884\r\n",
      "epoch: 54, batch_id: 0, loss is: 1.0527184713282622e-05\r\n",
      "epoch: 54, batch_id: 50, loss is: 2.7568839868763462e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.1798080205917358\r\n",
      "epoch: 55, batch_id: 0, loss is: 1.91620292753214e-05\r\n",
      "epoch: 55, batch_id: 50, loss is: 2.6261757739121094e-05\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.1886335611343384\r\n",
      "epoch: 56, batch_id: 0, loss is: 3.771553747355938e-05\r\n",
      "epoch: 56, batch_id: 50, loss is: 1.5109270862012636e-05\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 1.1941146850585938\r\n",
      "epoch: 57, batch_id: 0, loss is: 1.5440820789081044e-05\r\n",
      "epoch: 57, batch_id: 50, loss is: 1.3380891687120311e-05\r\n",
      "[validation] accuracy: 0.8497023582458496, loss: 1.1944682598114014\r\n",
      "epoch: 58, batch_id: 0, loss is: 3.459665094851516e-05\r\n",
      "epoch: 58, batch_id: 50, loss is: 1.7236208805115893e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.2105059623718262\r\n",
      "epoch: 59, batch_id: 0, loss is: 1.1518232895468827e-05\r\n",
      "epoch: 59, batch_id: 50, loss is: 1.8262471712660044e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.193963646888733\r\n",
      "epoch: 60, batch_id: 0, loss is: 9.56627081905026e-06\r\n",
      "epoch: 60, batch_id: 50, loss is: 1.783229163265787e-05\r\n",
      "[validation] accuracy: 0.8482142686843872, loss: 1.2089543342590332\r\n",
      "epoch: 61, batch_id: 0, loss is: 9.607310857973062e-06\r\n",
      "epoch: 61, batch_id: 50, loss is: 7.610626653331565e-06\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.2327537536621094\r\n",
      "epoch: 62, batch_id: 0, loss is: 1.7381644283887e-05\r\n",
      "epoch: 62, batch_id: 50, loss is: 1.3764462892140727e-05\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.222637414932251\r\n",
      "epoch: 63, batch_id: 0, loss is: 2.3071574105415493e-05\r\n",
      "epoch: 63, batch_id: 50, loss is: 1.34359816001961e-05\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.248753547668457\r\n",
      "epoch: 64, batch_id: 0, loss is: 1.9311202777316794e-05\r\n",
      "epoch: 64, batch_id: 50, loss is: 1.6490861526108347e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.2554675340652466\r\n",
      "epoch: 65, batch_id: 0, loss is: 1.1808844647021033e-05\r\n",
      "epoch: 65, batch_id: 50, loss is: 1.5325291315093637e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.2575968503952026\r\n",
      "epoch: 66, batch_id: 0, loss is: 7.547308996436186e-06\r\n",
      "epoch: 66, batch_id: 50, loss is: 1.2695332770817913e-05\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.259175181388855\r\n",
      "epoch: 67, batch_id: 0, loss is: 1.8863840523408726e-05\r\n",
      "epoch: 67, batch_id: 50, loss is: 5.181803771847626e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.2764651775360107\r\n",
      "epoch: 68, batch_id: 0, loss is: 4.932208867103327e-06\r\n",
      "epoch: 68, batch_id: 50, loss is: 1.599216921022162e-05\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.2712609767913818\r\n",
      "epoch: 69, batch_id: 0, loss is: 2.0789691916434094e-05\r\n",
      "epoch: 69, batch_id: 50, loss is: 1.0355955964769237e-05\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.2503801584243774\r\n",
      "epoch: 70, batch_id: 0, loss is: 9.611061614123173e-06\r\n",
      "epoch: 70, batch_id: 50, loss is: 1.4062479749554768e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.2839298248291016\r\n",
      "epoch: 71, batch_id: 0, loss is: 1.077703018381726e-05\r\n",
      "epoch: 71, batch_id: 50, loss is: 1.2162854545749724e-05\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.3054609298706055\r\n",
      "epoch: 72, batch_id: 0, loss is: 1.3246830349089578e-05\r\n",
      "epoch: 72, batch_id: 50, loss is: 8.51917047839379e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.3119661808013916\r\n",
      "epoch: 73, batch_id: 0, loss is: 5.714492090191925e-06\r\n",
      "epoch: 73, batch_id: 50, loss is: 1.3719211892748717e-05\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.2969361543655396\r\n",
      "epoch: 74, batch_id: 0, loss is: 2.231429562016274e-06\r\n",
      "epoch: 74, batch_id: 50, loss is: 9.256842531613074e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.3268227577209473\r\n",
      "epoch: 75, batch_id: 0, loss is: 1.851453816925641e-05\r\n",
      "epoch: 75, batch_id: 50, loss is: 7.882623322075233e-06\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.3204848766326904\r\n",
      "epoch: 76, batch_id: 0, loss is: 4.783213626069482e-06\r\n",
      "epoch: 76, batch_id: 50, loss is: 1.1797452316386625e-05\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.3402093648910522\r\n",
      "epoch: 77, batch_id: 0, loss is: 1.1707848898367956e-05\r\n",
      "epoch: 77, batch_id: 50, loss is: 1.0233206921839155e-05\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.3461532592773438\r\n",
      "epoch: 78, batch_id: 0, loss is: 6.120586022007046e-06\r\n",
      "epoch: 78, batch_id: 50, loss is: 8.743083526496775e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.351589322090149\r\n",
      "epoch: 79, batch_id: 0, loss is: 6.243507414183114e-06\r\n",
      "epoch: 79, batch_id: 50, loss is: 1.1343174264766276e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.3592147827148438\r\n",
      "epoch: 80, batch_id: 0, loss is: 9.491890523349866e-06\r\n",
      "epoch: 80, batch_id: 50, loss is: 1.8319045921089128e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.3659694194793701\r\n",
      "epoch: 81, batch_id: 0, loss is: 5.334552952263039e-06\r\n",
      "epoch: 81, batch_id: 50, loss is: 5.85981433687266e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.3743699789047241\r\n",
      "epoch: 82, batch_id: 0, loss is: 7.055421519908123e-06\r\n",
      "epoch: 82, batch_id: 50, loss is: 1.4713693417434115e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.3796427249908447\r\n",
      "epoch: 83, batch_id: 0, loss is: 5.051462721894495e-06\r\n",
      "epoch: 83, batch_id: 50, loss is: 1.1808478120656218e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.3614686727523804\r\n",
      "epoch: 84, batch_id: 0, loss is: 5.122214133734815e-06\r\n",
      "epoch: 84, batch_id: 50, loss is: 6.988560926401988e-06\r\n",
      "[validation] accuracy: 0.8467261791229248, loss: 1.3411715030670166\r\n",
      "epoch: 85, batch_id: 0, loss is: 2.384166919000563e-06\r\n",
      "epoch: 85, batch_id: 50, loss is: 4.786871613760013e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.4024710655212402\r\n",
      "epoch: 86, batch_id: 0, loss is: 1.9408621483307797e-06\r\n",
      "epoch: 86, batch_id: 50, loss is: 6.180041054903995e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.3964471817016602\r\n",
      "epoch: 87, batch_id: 0, loss is: 1.248237367690308e-05\r\n",
      "epoch: 87, batch_id: 50, loss is: 9.700461305328645e-06\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.395690679550171\r\n",
      "epoch: 88, batch_id: 0, loss is: 1.9445919861027505e-06\r\n",
      "epoch: 88, batch_id: 50, loss is: 6.630875759583432e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.4217058420181274\r\n",
      "epoch: 89, batch_id: 0, loss is: 3.1813804071134655e-06\r\n",
      "epoch: 89, batch_id: 50, loss is: 3.5240843772044173e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.4114034175872803\r\n",
      "epoch: 90, batch_id: 0, loss is: 1.493821628173464e-06\r\n",
      "epoch: 90, batch_id: 50, loss is: 3.248425400670385e-06\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.3875174522399902\r\n",
      "epoch: 91, batch_id: 0, loss is: 5.662318926624721e-06\r\n",
      "epoch: 91, batch_id: 50, loss is: 1.6289646737277508e-05\r\n",
      "[validation] accuracy: 0.84375, loss: 1.4220740795135498\r\n",
      "epoch: 92, batch_id: 0, loss is: 4.675192940339912e-06\r\n",
      "epoch: 92, batch_id: 50, loss is: 3.170197942381492e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.4437625408172607\r\n",
      "epoch: 93, batch_id: 0, loss is: 1.5757918845338281e-06\r\n",
      "epoch: 93, batch_id: 50, loss is: 3.8966218198766e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.4527969360351562\r\n",
      "epoch: 94, batch_id: 0, loss is: 5.978986337140668e-06\r\n",
      "epoch: 94, batch_id: 50, loss is: 8.344635489265784e-07\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.4603376388549805\r\n",
      "epoch: 95, batch_id: 0, loss is: 2.0787049379578093e-06\r\n",
      "epoch: 95, batch_id: 50, loss is: 5.762866749137174e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.4684537649154663\r\n",
      "epoch: 96, batch_id: 0, loss is: 2.4065257093752734e-06\r\n",
      "epoch: 96, batch_id: 50, loss is: 1.482660991314333e-06\r\n",
      "[validation] accuracy: 0.8452380895614624, loss: 1.4613258838653564\r\n",
      "epoch: 97, batch_id: 0, loss is: 1.6279468582069967e-06\r\n",
      "epoch: 97, batch_id: 50, loss is: 2.108504304487724e-06\r\n",
      "[validation] accuracy: 0.8422619104385376, loss: 1.4807554483413696\r\n",
      "epoch: 98, batch_id: 0, loss is: 2.138305490007042e-06\r\n",
      "epoch: 98, batch_id: 50, loss is: 1.206988827107125e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.47488272190094\r\n",
      "epoch: 99, batch_id: 0, loss is: 1.6502981452504173e-06\r\n",
      "epoch: 99, batch_id: 50, loss is: 4.451660515769618e-06\r\n",
      "[validation] accuracy: 0.84375, loss: 1.4579557180404663\r\n",
      "running_time is 40.5986909866333\r\n",
      "[validation] accuracy: 0.84375, loss: 1.473444938659668\r\n",
      "acc:0.8631,pre:0.8646,rec:0.8696,f1:0.8671\r\n",
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "           0     0.8615    0.8563    0.8589       327\r\n",
      "           1     0.8646    0.8696    0.8671       345\r\n",
      "\r\n",
      "    accuracy                         0.8631       672\r\n",
      "   macro avg     0.8630    0.8629    0.8630       672\r\n",
      "weighted avg     0.8631    0.8631    0.8631       672\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Conv(dict_dim=vocab[\"<pad>\"])\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "train(cnn_model, epochs, \"cnn\")\n",
    "end_time=time.time()\n",
    "running_time=end_time-start_time\n",
    "print(\"running_time is {}\".format(running_time))\n",
    "\n",
    "model_state_dict = paddle.load('cnn_model_final.pdparams')\n",
    "cnn_model.set_state_dict(model_state_dict) \n",
    "cnn_model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    logits = cnn_model(sent)\n",
    "    loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "    acc = paddle.metric.accuracy(logits, label)\n",
    "    \n",
    "    accuracies.append(acc.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "model_state_dict = paddle.load('model_final.pdparams')\n",
    "transformer_model.set_state_dict(model_state_dict) \n",
    "transformer_model.eval()\n",
    "predictions = []\n",
    "r = []\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    gt_labels = data[1].numpy()\n",
    "    for i in gt_labels:\n",
    "        r.append(i)\n",
    "    results = transformer_model(sent)\n",
    "    for probs in results:\n",
    "        # 映射分类label\n",
    "        idx = np.argmax(probs)\n",
    "        predictions.append(idx)\n",
    "    \n",
    "confusion_matrix(r, predictions)\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"0\",\"1\"]\n",
    "acc = accuracy_score(r, predictions).round(4)\n",
    "pre = precision_score(r, predictions).round(4)\n",
    "rec = recall_score(r, predictions).round(4)\n",
    "F1 = f1_score(r, predictions).round(4)\n",
    "print('acc:{},pre:{},rec:{},f1:{}'.format(acc,pre,rec,F1))\n",
    "CR=classification_report(r, predictions, target_names=target_names,digits=4)\n",
    "print(CR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
